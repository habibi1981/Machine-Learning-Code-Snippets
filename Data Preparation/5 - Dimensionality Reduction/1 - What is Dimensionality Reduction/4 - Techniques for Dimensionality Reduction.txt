There are many techniques that can be used for dimensionality reduction. In this section, we will review the main techniques:
1 - Feature Selection Methods:
    Perhaps the most common are so-called feature selection techniques that use scoring or statistical 
    methods to select which features to keep and which features to delete.
    perform feature selection, to remove "irrelevant" features that do not help much with the classification problem.
    Two main classes of feature selection techniques include wrapper methods and filter methods.
    Wrapper methods, as the name suggests, wrap a machine learning model, fitting and evaluating 
    the model with different subsets of input features and selecting the subset the results in the best model performance. 
    RFE is an example of a wrapper feature selection method. Filter methods
    use scoring methods, like correlation between the feature and the target variable, 
    to select a subset of input features that are most predictive. Examples include Pearson's correlation and Chi-Squared test.

2 - Matrix Factorization:
    Techniques from linear algebra can be used for dimensionality reduction. Specifically, matrix
    factorization methods can be used to reduce a dataset matrix into its constituent parts. Examples
    include the eigendecomposition and singular value decomposition. The parts can then be ranked
    and a subset of those parts can be selected that best captures the salient structure of the
    matrix that can be used to represent the dataset. The most common method for ranking the
    components is principal components analysis, or PCA for short.
    The most common approach to dimensionality reduction is called principal compo-nents analysis or PCA.
 
3 - Manifold Learning:
    Techniques from high-dimensionality statistics can also be used for dimensionality reduction.
    In mathematics, a projection is a kind of function or mapping that transforms data in some way.
    These techniques are sometimes referred to as manifold learning and are used to create a
    low-dimensional projection of high-dimensional data, often for the purposes of data visualization.
    The projection is designed to both create a low-dimensional representation of the dataset whilst
    best preserving the salient structure or relationships in the data. Examples of manifold learning
    techniques include:
     Kohonen Self-Organizing Map (SOM).
     Sammons Mapping
     Multidimensional Scaling (MDS)
     t-distributed Stochastic Neighbor Embedding (t-SNE).
    The features in the projection often have little relationship with the original columns, 
    e.g. they do not have column names, which can be confusing to beginners.

4- Autoencoder Methods:
    Deep learning neural networks can be constructed to perform dimensionality reduction. A
    popular approach is called autoencoders. This involves framing a self-supervised learning
    problem where a model must reproduce the input correctly. A network model is used that seeks
    to compress the data  ow to a bottleneck layer with far fewer dimensions than the original
    input data. The part of the model prior to and including the bottleneck is referred to as the
    encoder, and the part of the model that reads the bottleneck output and reconstructs the input
    is called the decoder.
    An auto-encoder is a kind of unsupervised neural network that is used for dimen-sionality 
    reduction and feature discovery. More precisely, an auto-encoder is a
    feedforward neural network that is trained to predict the input itself.
    After training, the decoder is discarded and the output from the bottleneck is used directly
    as the reduced dimensionality of the input. Inputs transformed by this encoder can then be fed
    into another model, not necessarily a neural network model.
    Deep autoencoders are an effective framework for nonlinear dimensionality reduction.
    Once such a network has been built, the top-most layer of the encoder, the code
    layer hc, can be input to a supervised classification procedure.
    The output of the encoder is a type of projection, and like other projection methods, there
    is no direct relationship to the bottleneck output back to the original input variables, making
    them challenging to interpret.

